{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration with human and mouse lung cell atlas\n",
    "Will use [SATURN](https://github.com/snap-stanford/saturn) that uses a \"macrogene\" concept that incorporates protein sequence similarities to better bridge cross-species relationships.  \n",
    "\n",
    "Need the `saturn` conda environment, generated by running `conda env create -f saturn-env.yml`  \n",
    "\n",
    "Also downloaded the protein embedding files from: http://snap.stanford.edu/saturn/data/protein_embeddings.tar.gz  \n",
    "\n",
    "Prefiltered data files for the lung cell atlases will be downloaded programmatically.\n",
    "\n",
    "As a first test, try to integrate the Tata basal cell data.\n",
    "\n",
    "#### Fetal lung atlas  \n",
    "https://fetal-lung.cellgeni.sanger.ac.uk/scRNA.html  \n",
    "scRNA-seq data: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE264407   \n",
    "hPSC-derived lung (~fetal) organoids: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE266789  \n",
    "\n",
    "\n",
    "### Workflow\n",
    "\n",
    "#### SATURN takes as an input:\n",
    "\n",
    "* multiple scRNA-seq count datasets from different species (AnnDatas), with cell type annotations.\n",
    "* protein embeddings generated by large language models (TorchDicts)\n",
    "\n",
    "#### SATURN is composed of three modules:\n",
    "* Macrogene initialization with Kmeans (`scipy`)\n",
    "* Pretraining conditional autoencoder (`scVI ZINB loss`)\n",
    "* Fine tuning cell clusters with weakly supervised metric learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load required Python libraries\n",
    "\n",
    "For loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from anndata import AnnData\n",
    "from anndata.io import read_csv\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define path to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = Path('/Volumes/Hackathon1/DukeSeq/LungAtlas')\n",
    "DATADIR2 = Path(DATADIR,\"../Basal_organoids_scRNAseq\")\n",
    "PROCESS_BASAL = False # Set to True to extract primary basal cell data from allograft + Tata data h5ad file\n",
    "\n",
    "if not DATADIR.exists() or not DATADIR2.exists():\n",
    "    raise FileNotFoundError(f\"Data directory {DATADIR} or {DATADIR2} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If data don't exist, download\n",
    "For SATURN, must use gene names rather than ENSEMBL gene ids; need to change `var_names` as appropriate and save as new adata object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_hla_path = Path(DATADIR, \"b351804c-293e-4aeb-9c4c-043db67f4540.h5ad\")\n",
    "hla_url = 'https://datasets.cellxgene.cziscience.com/b351804c-293e-4aeb-9c4c-043db67f4540.h5ad'\n",
    "mod_hla_path = Path(DATADIR, \"human_lung_atlas.h5ad\")\n",
    "\n",
    "if not orig_hla_path.exists():\n",
    "    os.system(f'wget -O {hla_path} {hla_url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not mod_hla_path.exists():\n",
    "    hla = sc.read_h5ad(orig_hla_path)\n",
    "    if 'hla_ensembl_gene_id' not in globals():\n",
    "        hla_ensembl_gene_id = hla.var_names\n",
    "        hla.var['ensembl_gene_id'] = hla_ensembl_gene_id\n",
    "        hla.var_names = hla.var['feature_name']\n",
    "    sc.pl.umap(hla, color=['cell_type','scanvi_label',], ncols=1)\n",
    "    print(f\"Saving modified h5ad file to {mod_hla_path}\")\n",
    "    hla.write_h5ad(mod_hla_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tata mouse basal cell data found. Loading file /Volumes/Hackathon1/DukeSeq/LungAtlas/Tata_basal_cells_20250214.h5ad\n"
     ]
    }
   ],
   "source": [
    "tata_path = Path(DATADIR, \"Tata_basal_cells_20250214.h5ad\")\n",
    "basal_path = Path(DATADIR2, \"010325_adata2_basal_cells_orgs_allos.h5ad\")\n",
    "\n",
    "if tata_path.exists():\n",
    "    print(f\"Tata mouse basal cell data found. Loading file {tata_path}\")\n",
    "    primary_basal = sc.read_h5ad(tata_path)\n",
    "else:\n",
    "    print(f\"Tata data file {tata_path} not found. Will process from other data file\")\n",
    "    if not basal_path.exists():\n",
    "        raise FileNotFoundError(f\"Data file {basal_path} not found.\")\n",
    "    else:\n",
    "        PROCESS_BASAL = True\n",
    "        basal = sc.read_h5ad(basal_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_BASAL:\n",
    "    print(\"Processing basal cell data\")\n",
    "    basal.obs = basal.obs[['Model','cluster','UnID']].copy()\n",
    "    basal.var = basal.var[['mito','total_counts']].copy()\n",
    "\n",
    "    keep_uns = ['UnID_colors','leiden','neighbors','umap']\n",
    "    uns_list = [basal.uns[x] for x in basal.uns.keys() if x in keep_uns]\n",
    "    basal.uns = dict(zip(keep_uns, uns_list))\n",
    "\n",
    "    del basal.obsm['X_scVI_1.1']\n",
    "    del basal.obsm['X_scVI_1.2']\n",
    "    del basal.obsm['_scvi_extra_continuous_covs']\n",
    "    del basal.varm\n",
    "\n",
    "    prim_mask = basal.obs['UnID'].str.contains('Primary')\n",
    "    basal = basal[prim_mask].copy()\n",
    "\n",
    "    sc.pl.umap(basal, color=['cluster','Model'], frameon=False, ncols=2)\n",
    "\n",
    "    basal.obs['cell_type'] = ['Basal'] * basal.shape[0]\n",
    "    basal.write_h5ad(tata_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following example notebook\n",
    "https://github.com/snap-stanford/SATURN/blob/main/Vignettes/frog_zebrafish_embryogenesis/Train%20SATURN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>species</th>\n",
       "      <th>embedding_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Volumes/Hackathon1/DukeSeq/LungAtlas/human_lu...</td>\n",
       "      <td>human</td>\n",
       "      <td>/Volumes/Hackathon1/DukeSeq/SATURN/protein_emb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Volumes/Hackathon1/DukeSeq/LungAtlas/Tata_bas...</td>\n",
       "      <td>mouse</td>\n",
       "      <td>/Volumes/Hackathon1/DukeSeq/SATURN/protein_emb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path species  \\\n",
       "0  /Volumes/Hackathon1/DukeSeq/LungAtlas/human_lu...   human   \n",
       "1  /Volumes/Hackathon1/DukeSeq/LungAtlas/Tata_bas...   mouse   \n",
       "\n",
       "                                      embedding_path  \n",
       "0  /Volumes/Hackathon1/DukeSeq/SATURN/protein_emb...  \n",
       "1  /Volumes/Hackathon1/DukeSeq/SATURN/protein_emb...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_path_base = Path('/Volumes/Hackathon1/DukeSeq/SATURN/protein_embeddings_export/protXL')\n",
    "df = pd.DataFrame(columns=[\"path\", \"species\", \"embedding_path\"])\n",
    "df[\"species\"] = [\"human\", \"mouse\"]\n",
    "df[\"path\"] = [mod_hla_path, tata_path]\n",
    "df[\"embedding_path\"] = [Path(embed_path_base,\"human_embedding.torch\"), Path(embed_path_base,\"mouse_embedding.torch\")]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(DATADIR, \"SATURN_primary_basal_HLA_20250214.csv\").exists():\n",
    "    df.to_csv(Path(DATADIR, \"SATURN_primary_basal_HLA_20250214.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SATURN_RUN_INFO=/Volumes/Hackathon1/DukeSeq/LungAtlas/SATURN_primary_basal_HLA_20250214.csv\n",
      "Using MPS Device (detected Apple silicon)\n",
      "Using Device 0\n",
      "Set seed to 0\n",
      "After loading the anndata human View of AnnData object with n_obs × n_vars = 584944 × 18072\n",
      "    obs: 'suspension_type', 'donor_id', 'is_primary_data', 'assay_ontology_term_id', 'cell_type_ontology_term_id', 'development_stage_ontology_term_id', 'disease_ontology_term_id', 'self_reported_ethnicity_ontology_term_id', 'tissue_ontology_term_id', 'organism_ontology_term_id', 'sex_ontology_term_id', 'BMI', 'age_or_mean_of_age_range', 'age_range', 'anatomical_region_ccf_score', 'ann_coarse_for_GWAS_and_modeling', 'ann_finest_level', 'ann_level_1', 'ann_level_2', 'ann_level_3', 'ann_level_4', 'ann_level_5', 'cause_of_death', 'dataset', 'entropy_dataset_leiden_3', 'entropy_original_ann_level_1_leiden_3', 'entropy_original_ann_level_2_clean_leiden_3', 'entropy_original_ann_level_3_clean_leiden_3', 'entropy_subject_ID_leiden_3', 'fresh_or_frozen', 'leiden_1', 'leiden_2', 'leiden_3', 'leiden_4', 'leiden_5', 'log10_total_counts', 'lung_condition', 'mixed_ancestry', 'n_genes_detected', 'original_ann_highest_res', 'original_ann_level_1', 'original_ann_level_2', 'original_ann_level_3', 'original_ann_level_4', 'original_ann_level_5', 'original_ann_nonharmonized', 'reannotation_type', 'reference_genome', 'sample', 'scanvi_label', 'sequencing_platform', 'size_factors', 'smoking_status', 'study', 'subject_type', 'tissue_dissociation_protocol', 'tissue_level_2', 'tissue_level_3', 'tissue_sampling_method', 'tissue_type', 'cell_type', 'assay', 'disease', 'organism', 'sex', 'tissue', 'self_reported_ethnicity', 'development_stage', 'observation_joinid', 'species', 'species_type_label', 'truth_labels', 'ref_labels'\n",
      "    var: 'feature_is_filtered', 'feature_name', 'feature_reference', 'feature_biotype', 'feature_length', 'feature_type', 'ensembl_gene_id'\n",
      "    uns: 'batch_condition', 'cell_type_colors', 'citation', 'default_embedding', 'scanvi_label_colors', 'schema_reference', 'schema_version', 'title'\n",
      "    obsm: 'X_scanvi_emb', 'X_umap'\n",
      "    obsp: 'connectivities', 'distances'\n",
      "After loading the anndata mouse View of AnnData object with n_obs × n_vars = 680 × 22325\n",
      "    obs: 'Model', 'cluster', 'UnID', 'cell_type', 'species', 'species_type_label', 'truth_labels', 'ref_labels'\n",
      "    var: 'mito', 'total_counts'\n",
      "    uns: 'Model_colors', 'UnID_colors', 'cluster_colors', 'leiden', 'neighbors', 'umap'\n",
      "    obsm: 'X_pca', 'X_umap'\n",
      "    layers: 'counts', 'norm'\n",
      "    obsp: 'connectivities', 'distances'\n",
      "Making Centroids\n",
      "Pretraining...\n",
      "  0%|                                                   | 0/200 [00:07<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/darren/git-repos/SATURN/Vignettes/human_mouse_lung/../../train-saturn.py\", line 1068, in <module>\n",
      "    trainer(args)\n",
      "  File \"/Users/darren/git-repos/SATURN/Vignettes/human_mouse_lung/../../train-saturn.py\", line 654, in trainer\n",
      "    pretrain_model = pretrain_saturn(pretrain_model, pretrain_loader, optim_pretrain, \n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/darren/git-repos/SATURN/Vignettes/human_mouse_lung/../../train-saturn.py\", line 203, in pretrain_saturn\n",
      "    (data, labels, refs, batch_labels) = batch_dict[species]\n",
      "                                         ~~~~~~~~~~^^^^^^^^^\n",
      "KeyError: 'mouse'\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "run_info_path = Path(DATADIR, \"SATURN_primary_basal_HLA_20250214.csv\")\n",
    "%env SATURN_RUN_INFO=$run_info_path\n",
    "\n",
    "!python3 ../../train-saturn.py --in_data=$SATURN_RUN_INFO \\\n",
    "                              --in_label_col=cell_type --ref_label_col=cell_type \\\n",
    "                              --num_macrogenes=2000     --hv_genes=8000          \\\n",
    "                            #   --centroids_init_path=saturn_results/hm_centroids.pkl \\\n",
    "                              --work_dir=. \\\n",
    "                              --device_num=7 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: train-saturn.py [-h] [--in_data IN_DATA] [--device DEVICE]\n",
      "                       [--device_num DEVICE_NUM] [--time_stamp TIME_STAMP]\n",
      "                       [--org ORG] [--log_dir LOG_DIR] [--work_dir WORK_DIR]\n",
      "                       [--seed SEED] [--in_label_col IN_LABEL_COL]\n",
      "                       [--ref_label_col REF_LABEL_COL]\n",
      "                       [--tissue_subset TISSUE_SUBSET]\n",
      "                       [--tissue_column TISSUE_COLUMN] [--hv_genes HV_GENES]\n",
      "                       [--hv_span HV_SPAN] [--num_macrogenes NUM_MACROGENES]\n",
      "                       [--centroids_init_path CENTROIDS_INIT_PATH]\n",
      "                       [--embedding_model {ESM1b,MSA1b,protXL,ESM1b_protref,ESM2}]\n",
      "                       [--centroid_score_func {default,one_hot,smoothed}]\n",
      "                       [--vae [VAE]] [--hidden_dim HIDDEN_DIM]\n",
      "                       [--model_dim MODEL_DIM]\n",
      "                       [--binarize_expression [BINARIZE_EXPRESSION]]\n",
      "                       [--scale_expression [SCALE_EXPRESSION]]\n",
      "                       [--pretrain [PRETRAIN]]\n",
      "                       [--pretrain_model_path PRETRAIN_MODEL_PATH]\n",
      "                       [--pretrain_lr PRETRAIN_LR]\n",
      "                       [--pretrain_batch_size PRETRAIN_BATCH_SIZE]\n",
      "                       [--l1_penalty L1_PENALTY]\n",
      "                       [--pe_sim_penalty PE_SIM_PENALTY]\n",
      "                       [--pretrain_epochs PRETRAIN_EPOCHS]\n",
      "                       [--unfreeze_macrogenes [UNFREEZE_MACROGENES]]\n",
      "                       [--mnn [MNN]] [--use_ref_labels [USE_REF_LABELS]]\n",
      "                       [--batch_size BATCH_SIZE] [--metric_lr METRIC_LR]\n",
      "                       [--epochs EPOCHS]\n",
      "                       [--balance_pretrain [BALANCE_PRETRAIN]]\n",
      "                       [--equalize_triplets_species [EQUALIZE_TRIPLETS_SPECIES]]\n",
      "                       [--balance_species_cells [BALANCE_SPECIES_CELLS]]\n",
      "                       [--non_species_batch_col NON_SPECIES_BATCH_COL]\n",
      "                       [--polling_freq POLLING_FREQ]\n",
      "                       [--score_adatas [SCORE_ADATAS]]\n",
      "                       [--ct_map_path CT_MAP_PATH]\n",
      "                       [--score_ref_labels [SCORE_REF_LABELS]]\n",
      "\n",
      "Set model hyperparametrs.\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --in_data IN_DATA     Path to csv containing input datas and species\n",
      "                        (default: None)\n",
      "  --device DEVICE       Set GPU/CPU/MPS (default: cpu)\n",
      "  --device_num DEVICE_NUM\n",
      "                        Set GPU Number (default: 0)\n",
      "  --time_stamp TIME_STAMP\n",
      "                        Add time stamp in file name (default: False)\n",
      "  --org ORG             Add organization to filename (default: saturn)\n",
      "  --log_dir LOG_DIR     Log directory (default: tboard_log/)\n",
      "  --work_dir WORK_DIR   Working directory (default: ./out/)\n",
      "  --seed SEED           Init Seed (default: 0)\n",
      "  --in_label_col IN_LABEL_COL\n",
      "                        Label column for input data (default: None)\n",
      "  --ref_label_col REF_LABEL_COL\n",
      "                        Reference label column for input data (default:\n",
      "                        cell_type)\n",
      "  --tissue_subset TISSUE_SUBSET\n",
      "                        Subset the input anndatas by the column\n",
      "                        args.tissue_column to just be this tissue (default:\n",
      "                        None)\n",
      "  --tissue_column TISSUE_COLUMN\n",
      "                        When subsetting the input anndatas by the column, use\n",
      "                        this column name. (default: tissue_type)\n",
      "  --hv_genes HV_GENES   Number of highly variable genes (default: 8000)\n",
      "  --hv_span HV_SPAN     Fraction of cells to use when calculating highly\n",
      "                        variable genes, scanpy defeault is 0.3. (default: 0.3)\n",
      "  --num_macrogenes NUM_MACROGENES\n",
      "                        Number of macrogenes (default: 2000)\n",
      "  --centroids_init_path CENTROIDS_INIT_PATH\n",
      "                        Path to existing centroids pretraining weights, or\n",
      "                        location to save to. (default: None)\n",
      "  --embedding_model {ESM1b,MSA1b,protXL,ESM1b_protref,ESM2}\n",
      "                        Gene embedding model whose embeddings should be loaded\n",
      "                        if using gene_embedding_method (default: ESM1b)\n",
      "  --centroid_score_func {default,one_hot,smoothed}\n",
      "                        Gene embedding model whose embeddings should be loaded\n",
      "                        if using gene_embedding_method (default: default)\n",
      "  --vae [VAE]           Set the embedding layer to be a VAE. (default: False)\n",
      "  --hidden_dim HIDDEN_DIM\n",
      "                        Model first layer hidden dimension (default: 256)\n",
      "  --model_dim MODEL_DIM\n",
      "                        Model latent space dimension (default: 256)\n",
      "  --binarize_expression [BINARIZE_EXPRESSION]\n",
      "                        Whether to binarize the gene expression matrix\n",
      "                        (default: False)\n",
      "  --scale_expression [SCALE_EXPRESSION]\n",
      "                        Whether to scale the gene expression to zero mean and\n",
      "                        unit variance (default: False)\n",
      "  --pretrain [PRETRAIN]\n",
      "                        Pretrain the model (default: True)\n",
      "  --pretrain_model_path PRETRAIN_MODEL_PATH\n",
      "                        Path to save/load a pretraining model to (default:\n",
      "                        None)\n",
      "  --pretrain_lr PRETRAIN_LR\n",
      "                        Pre training Learning learning rate (default: 0.0005)\n",
      "  --pretrain_batch_size PRETRAIN_BATCH_SIZE\n",
      "                        pretrain batch size (default: 4096)\n",
      "  --l1_penalty L1_PENALTY\n",
      "                        L1 Penalty hyperparameter Default is 0. (default: 0.0)\n",
      "  --pe_sim_penalty PE_SIM_PENALTY\n",
      "                        Protein Embedding similarity to Macrogene loss, weight\n",
      "                        hyperparameter. Default is 1.0 (default: 0.2)\n",
      "  --pretrain_epochs PRETRAIN_EPOCHS\n",
      "                        Number of pretraining epochs (default: 200)\n",
      "  --unfreeze_macrogenes [UNFREEZE_MACROGENES]\n",
      "                        Let Metric Learning Modify macrogene weights (default:\n",
      "                        False)\n",
      "  --mnn [MNN]           Use mutual nearest neighbors (default: True)\n",
      "  --use_ref_labels [USE_REF_LABELS]\n",
      "                        Use reference labels when aligning (default: False)\n",
      "  --batch_size BATCH_SIZE\n",
      "                        batch size (default: 4096)\n",
      "  --metric_lr METRIC_LR\n",
      "                        Metric Learning learning rate (default: 0.001)\n",
      "  --epochs EPOCHS       Number of epochs for metric learning (default: 50)\n",
      "  --balance_pretrain [BALANCE_PRETRAIN]\n",
      "                        Balance cell types' weighting in the pretraining model\n",
      "                        (default: False)\n",
      "  --equalize_triplets_species [EQUALIZE_TRIPLETS_SPECIES]\n",
      "                        Balance species' weighting in the metric learning\n",
      "                        model (default: False)\n",
      "  --balance_species_cells [BALANCE_SPECIES_CELLS]\n",
      "                        Balance species' number of cells in all steps by\n",
      "                        resampling randomly. (default: False)\n",
      "  --non_species_batch_col NON_SPECIES_BATCH_COL\n",
      "                        Extra column for batch correction for pretraining. For\n",
      "                        example, the tissue column in atlas data. (default:\n",
      "                        None)\n",
      "  --polling_freq POLLING_FREQ\n",
      "                        Epoch Frequency of scoring during metric learning\n",
      "                        (default: 25)\n",
      "  --score_adatas [SCORE_ADATAS]\n",
      "                        Score the pretraining and metric learning adatas.\n",
      "                        (default: False)\n",
      "  --ct_map_path CT_MAP_PATH\n",
      "                        Path to csv containing label column mappings between\n",
      "                        species (default: )\n",
      "  --score_ref_labels [SCORE_REF_LABELS]\n",
      "                        Use the ref labels to score instead of the labels.\n",
      "                        (default: False)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python3 ../../train-saturn.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting SATURN code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load the saved state dictionary\n",
    "# hum_embed = torch.load(Path(embed_path_base,'human_embedding.torch'))\n",
    "# mouse_embed = torch.load(Path(embed_path_base,'mouse_embedding.torch'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Apple silicon (MPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MPS...\n",
      "Tensor on MPS: tensor([[0.6719, 0.7950, 0.5991],\n",
      "        [0.4259, 0.0201, 0.1482],\n",
      "        [0.7656, 0.2315, 0.5584]], device='mps:0')\n",
      "Tensor operation result on MPS: tensor([[1.3438, 1.5899, 1.1982],\n",
      "        [0.8517, 0.0402, 0.2964],\n",
      "        [1.5311, 0.4629, 1.1167]], device='mps:0')\n",
      "Selected device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(\"Testing MPS...\")\n",
    "    device = torch.device('mps')\n",
    "    x = torch.rand(3, 3)\n",
    "    x_mps = x.to('mps')\n",
    "    print(\"Tensor on MPS:\", x_mps)\n",
    "    y = x_mps * 2\n",
    "    print(\"Tensor operation result on MPS:\", y)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Selected device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(View of AnnData object with n_obs × n_vars = 584944 × 18072\n",
       "     obs: 'suspension_type', 'donor_id', 'is_primary_data', 'assay_ontology_term_id', 'cell_type_ontology_term_id', 'development_stage_ontology_term_id', 'disease_ontology_term_id', 'self_reported_ethnicity_ontology_term_id', 'tissue_ontology_term_id', 'organism_ontology_term_id', 'sex_ontology_term_id', 'BMI', 'age_or_mean_of_age_range', 'age_range', 'anatomical_region_ccf_score', 'ann_coarse_for_GWAS_and_modeling', 'ann_finest_level', 'ann_level_1', 'ann_level_2', 'ann_level_3', 'ann_level_4', 'ann_level_5', 'cause_of_death', 'dataset', 'entropy_dataset_leiden_3', 'entropy_original_ann_level_1_leiden_3', 'entropy_original_ann_level_2_clean_leiden_3', 'entropy_original_ann_level_3_clean_leiden_3', 'entropy_subject_ID_leiden_3', 'fresh_or_frozen', 'leiden_1', 'leiden_2', 'leiden_3', 'leiden_4', 'leiden_5', 'log10_total_counts', 'lung_condition', 'mixed_ancestry', 'n_genes_detected', 'original_ann_highest_res', 'original_ann_level_1', 'original_ann_level_2', 'original_ann_level_3', 'original_ann_level_4', 'original_ann_level_5', 'original_ann_nonharmonized', 'reannotation_type', 'reference_genome', 'sample', 'scanvi_label', 'sequencing_platform', 'size_factors', 'smoking_status', 'study', 'subject_type', 'tissue_dissociation_protocol', 'tissue_level_2', 'tissue_level_3', 'tissue_sampling_method', 'tissue_type', 'cell_type', 'assay', 'disease', 'organism', 'sex', 'tissue', 'self_reported_ethnicity', 'development_stage', 'observation_joinid'\n",
       "     var: 'feature_is_filtered', 'feature_name', 'feature_reference', 'feature_biotype', 'feature_length', 'feature_type', 'ensembl_gene_id'\n",
       "     uns: 'batch_condition', 'cell_type_colors', 'citation', 'default_embedding', 'scanvi_label_colors', 'schema_reference', 'schema_version', 'title'\n",
       "     obsm: 'X_scanvi_emb', 'X_umap'\n",
       "     obsp: 'connectivities', 'distances',\n",
       " {'human': tensor([[ 0.0389,  0.1122, -0.0067,  ..., -0.0383, -0.0164, -0.0059],\n",
       "          [ 0.0134, -0.0292,  0.0476,  ..., -0.0012,  0.0407, -0.0037],\n",
       "          [ 0.0378,  0.0122,  0.0302,  ..., -0.0058, -0.0434,  0.0055],\n",
       "          ...,\n",
       "          [-0.0045,  0.0403,  0.0350,  ...,  0.0512,  0.0621,  0.0346],\n",
       "          [-0.0256,  0.0308,  0.0188,  ...,  0.0449,  0.0510,  0.0531],\n",
       "          [ 0.0095, -0.1477,  0.0215,  ..., -0.0149,  0.0529,  0.1014]])})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../data')\n",
    "from gene_embeddings import load_gene_embeddings_adata\n",
    "\n",
    "def get_gene_embedding_path(species: list = ['human','mouse'], model: str = 'protXL', base_embedding_path: str = '') -> dict[str, Path]:\n",
    "    paths = []\n",
    "    for species_item in species:\n",
    "        paths.append(Path(base_embedding_path,model,f'{species_item}_embedding.torch'))\n",
    "    return dict(zip(species, paths))\n",
    "\n",
    "embed_path = '/Volumes/Hackathon1/DukeSeq/SATURN/protein_embeddings_export'\n",
    "pte = get_gene_embedding_path(['human','mouse'], base_embedding_path=embed_path)\n",
    "hla = sc.read_h5ad(mod_hla_path)\n",
    "load_gene_embeddings_adata(hla, ['human'], 'protXL', pte['human'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 680 × 55802\n",
       "    obs: 'Model', 'cluster', 'UnID', 'cell_type'\n",
       "    var: 'mito', 'total_counts'\n",
       "    uns: 'Model_colors', 'UnID_colors', 'cluster_colors', 'leiden', 'neighbors', 'umap'\n",
       "    obsm: 'X_pca', 'X_umap'\n",
       "    layers: 'counts', 'norm'\n",
       "    obsp: 'connectivities', 'distances'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primary_basal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['0610005C13Rik', '0610006L08Rik', '0610009B22Rik', '0610009E02Rik',\n",
       "       '0610009L18Rik', '0610010F05Rik', '0610010K14Rik', '0610012D04Rik',\n",
       "       '0610012G03Rik', '0610025J13Rik',\n",
       "       ...\n",
       "       'n-R5s90', 'n-R5s92', 'n-R5s93', 'n-R5s94', 'n-R5s95', 'n-R5s96',\n",
       "       'n-R5s97', 'n-R5s98', 'n-TSaga9', 'n-TStga1'],\n",
       "      dtype='object', length=55802)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primary_basal.var_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
